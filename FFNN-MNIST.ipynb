{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104dad26-a55f-41aa-8244-a972adbc18af",
   "metadata": {},
   "source": [
    "# MNIST Standard Biological Images: Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a937c8c3-67c6-46f4-bdd1-1d71f3cac62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns  # for nicer plots\n",
    "#sns.set(style=\"darkgrid\")  # default style\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "# Get date/time for local timezone\n",
    "def get_localdate_str():\n",
    "    tm_str = str(datetime.now())\n",
    "    return tm_str\n",
    "\n",
    "def get_time_with_minutes(date_str):\n",
    "    tm_str = re.sub('^....-..-.. ', \"\", date_str)\n",
    "    tm_str = re.sub(':..\\.......$', \"\", tm_str)\n",
    "    tm_str = re.sub(\":\", \"-\", tm_str)\n",
    "    return tm_str\n",
    "\n",
    "def get_date_with_seconds(date_str):\n",
    "    tm_str = re.sub('\\.......$', \"\", date_str)\n",
    "    tm_str = re.sub(\" \", \"_\", date_str)\n",
    "    return tm_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de3091-e2be-4ac9-8031-32fd6bd62879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(y_tst, y_prd, ttl=None, sz=5, lbl=None):\n",
    "    # Generate and display the confusion matrix\n",
    "    cm = confusion_matrix(y_tst, y_prd)\n",
    "\n",
    "    # Plot confusion matrix with labels\n",
    "    fig, ax = plt.subplots(figsize=(sz,sz))\n",
    "    \n",
    "    if (lbl != None):\n",
    "        im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Predicted labels')\n",
    "        ax.set_ylabel('True labels')\n",
    "\n",
    "        # Set tick labels\n",
    "        ax.set_xticks(np.arange(len(lbl)))\n",
    "        ax.set_yticks(np.arange(len(lbl)))\n",
    "        ax.set_xticklabels(lbl)\n",
    "        ax.set_yticklabels(lbl)\n",
    "\n",
    "        # Rotate x-tick labels\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "        # Add values to cells (optional)\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
    "    else:\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "\n",
    "    # Customize plot\n",
    "    plt.title(ttl)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ef206-5041-4e28-a501-3f5366f22f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_eval_graphs(ttl, epochs, trn_acc, val_acc, trn_loss, val_loss, xlab, ylab_acc, ylab_loss):\n",
    "   \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))    \n",
    "    fig.suptitle(ttl)\n",
    "    \n",
    "    ax1.plot(epochs, trn_acc, label='training accuracy')\n",
    "    ax1.plot(epochs, val_acc, label='validation accuracy')\n",
    "    #plt.plot  ( epochs, accuracy, label = 'training accuracy' )\n",
    "    #plt.plot  ( epochs, val_accuracy, label = 'validation accuracy' )\n",
    "    ax1.set_title(ylab_acc)\n",
    "    ax1.set_xlabel(xlab)\n",
    "    ax1.set_ylabel(ylab_acc)\n",
    "    ax1.legend(loc = 'best')\n",
    "\n",
    "    ax2.plot(epochs, trn_loss, label='training loss')\n",
    "    ax2.plot(epochs, val_loss, label='validation loss')\n",
    "    ax2.set_title(ylab_loss)\n",
    "    ax2.set_xlabel(xlab)\n",
    "    ax2.set_ylabel(ylab_loss)\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "    ax2.legend(loc = 'best')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcdbfa3-c6ea-48b0-b064-26627519c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set_dimensions(contained_array, data):\n",
    "    x_shape = data.shape\n",
    "    end_of_range = len(x_shape)\n",
    "    #print(\"end_of_range:\", end_of_range)\n",
    "\n",
    "    dim = \"\"\n",
    "    if end_of_range == 4:\n",
    "        if x_shape[3] == 3:\n",
    "            dim = \"RGB images:\"\n",
    "        else:\n",
    "            dim = \"3D images: \"\n",
    "        dim = dim + str(x_shape[1]) + \"x\" + str(x_shape[2]) + \"x\" + str(x_shape[3])\n",
    "    elif end_of_range == 3:\n",
    "        dim = \"2D images: \" + str(x_shape[1]) + \"x\" + str(x_shape[2])\n",
    "    if (dim != \"\"):\n",
    "        print(f\"  {contained_array}: {x_shape[0]} {dim}\")\n",
    "\n",
    "    return end_of_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624a570-d593-4ea3-a2b3-1fdc2bf84634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(contained_array, data, end_of_range):\n",
    "    data_shape = data.shape\n",
    "    \n",
    "    if (end_of_range > 2):\n",
    "        # Handle 2D, 3D, ... nD image dimensions\n",
    "        flat_sz = 1\n",
    "        for ix in range(1, end_of_range):\n",
    "            flat_sz = flat_sz * data_shape[ix]\n",
    "        #print(\"flat_sz:\", flat_sz)\n",
    "        data = data.reshape(-1, flat_sz)\n",
    "    else:\n",
    "        # Convert label dimensions to 1D\n",
    "        data = np.ravel(data, order='C')\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc322a7-3ee9-4fb4-afa5-0a439ae719b5",
   "metadata": {},
   "source": [
    "## Load data then flatten labels, images are flattened later within LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d3798-0f8c-45f2-a1c7-e68fe931b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_flatten(fn):\n",
    "    npz_file = np.load(fn, allow_pickle=True)\n",
    "    #print(fn, \"arrays:\", blood_npz.files)\n",
    "\n",
    "    # Dictionaries to store flattened image data\n",
    "    flat_arr = {}\n",
    "    label_arr = {}\n",
    "    # Dictionary to store images (unflattened)\n",
    "    orig_arr = {}\n",
    "    \n",
    "    substr = \"labels\"\n",
    "\n",
    "    for contained_array in npz_file.files:\n",
    "        # Only labels need to be flattened.\n",
    "        # Features (images) will be flattened by the Logistic Regression model\n",
    "        end_of_range = get_data_set_dimensions(contained_array, npz_file[contained_array])\n",
    "        match = re.search(substr, contained_array)\n",
    "        if (match):\n",
    "            #print(f\"  BEFORE: {contained_array}: {npz_file[contained_array].shape}\")\n",
    "            arr = flatten_data(contained_array, npz_file[contained_array], end_of_range)\n",
    "            label_arr[contained_array] = arr\n",
    "            #print(f\"  {contained_array} (flattened): {label_arr[contained_array].shape}\")\n",
    "        else:\n",
    "            orig_arr[contained_array] = npz_file[contained_array]\n",
    "    \n",
    "    return npz_file, orig_arr, label_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ea32f-9ae6-4da2-92a1-719b9a41c83e",
   "metadata": {},
   "source": [
    "## Build a Keras Logistic Regression model using a single Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d03eb-6e64-4844-977d-3ed925b3b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_classes,\n",
    "                img_dim,\n",
    "                hidden_layer_sizes=[],\n",
    "                activation='relu',\n",
    "                optimizer='adam',\n",
    "                learning_rate=0.01):\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    np.random.seed(0)\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    # Keras logistic regression has input and output layers with as a single Dense layer.\n",
    "    # So hidden_layer will be 0 for this model\n",
    "\n",
    "    hidden_layers = len(hidden_layer_sizes)\n",
    "    ix = 0\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=img_dim)),\n",
    "    model.add(Flatten())\n",
    "    while(hidden_layers != 0):\n",
    "        model.add(Dense(hidden_layer_sizes[ix], activation=activation))\n",
    "        ix += 1\n",
    "        hidden_layers -= 1\n",
    "    model.add(Dense(n_classes, activation='softmax')) # Output layer with n-classes\n",
    "    # Display the model summary to see the architecture\n",
    "    model.summary()\n",
    "\n",
    "    # Note:\n",
    "    # categorical_crossentropy\n",
    "    #   Expects the modelâ€™s output to be a tensor of predicted probabilities,\n",
    "    #   typically produced by applying a SoftMax activation function (multi-class\n",
    "    #   classification to the modelâ€™s final layer. The true labels are normally\n",
    "    #   one-hot encoded vectors.\n",
    "    # sparse_categorical_crossentropy\n",
    "    #   A special case of the CategoricalCrossentropy loss function, where the\n",
    "    #   labels are provided as integers instead of one-hot encoded vectors.\n",
    "    # categorical_focal_crossentropy\n",
    "    #   Loss function that is used for multi-class unbalanced classification tasks.\n",
    "    #   An extension to the standard Cross Entropy that addresses the issue of class\n",
    "    #   imbalance in certain classification tasks where the cross entropy is not enough.\n",
    "    #   It is good for multiclass classification where some classes are easy and others\n",
    "    #   difficult to classify.\n",
    "    \n",
    "    # Compile the model using the Adam optimizer\n",
    "    if (optimizer == 'adam'):\n",
    "        optimzer = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                #loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'],\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330641c-ecf7-410c-a0b8-51d6ae04646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_to_load, fn):\n",
    "\n",
    "    #result_dict = {}\n",
    "    csv_data = [[]]\n",
    "    print(\"\\n-------------------------------------------------------\")\n",
    "    print(fn)\n",
    "    npz_file, original_imgs, labels = load_and_flatten(file_to_load)\n",
    "    feature_keys = list(original_imgs.keys())\n",
    "    #print(feature_keys)\n",
    "    label_keys = list(labels.keys())\n",
    "    #print(label_keys)\n",
    "    \n",
    "    img_shape = original_imgs[feature_keys[0]].shape\n",
    "    img_dim = []\n",
    "    for ix in range(1, len(img_shape)):\n",
    "        img_dim.append(img_shape[ix])\n",
    "    #print(\"img_dim:\", img_dim)\n",
    "\n",
    "    X_train = original_imgs[feature_keys[0]]\n",
    "    y_train = labels[label_keys[0]]\n",
    "    X_test = original_imgs[feature_keys[2]]\n",
    "    y_test = labels[label_keys[2]]\n",
    "\n",
    "    # Find number of unique classes within data set\n",
    "    class_cnt, value_cnts = np.unique(labels[label_keys[0]], return_counts=True)\n",
    "    #print(f\"class_cnt: {class_cnt}\")\n",
    "    #print(f\"value_cnts: {value_cnts}\")\n",
    "    n_classes = len(class_cnt)\n",
    "    #print(f\"n_classes: {n_classes}\")\n",
    "\n",
    "    # Display the first image and label in the training data\n",
    "    print(\"\\nLabel: \", y_train[0])\n",
    "    if len(img_dim) < 3 or img_dim[2] == 3:\n",
    "        #img_arr = original_imgs[feature_keys[0]]\n",
    "        img = X_train[0]\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    lrate = [0.1, 0.01, 0.001]\n",
    "    #lrate = [0.01]\n",
    "    #epch = [25, 50, 100, 150]\n",
    "    epch = [25, 50, 100]\n",
    "\n",
    "    # Build model and train with a variety of learning rates and epochs for each data set\n",
    "    for learning_rate in lrate:\n",
    "        \n",
    "        for num_epochs in epch:\n",
    "\n",
    "            # Build the Feed Forward Neural Network Model\n",
    "            kerasFFNN = build_model(n_classes, img_dim, [32, 64, 128], 'relu', 'adam', learning_rate)\n",
    "            \n",
    "            ttl = fn + \": Epochs \" + str(num_epochs) + \", LR \" + str(learning_rate)\n",
    "            # Train the Logistic Regression model.\n",
    "            print(f'\\nTraining... {ttl}')\n",
    "            start = time.time()\n",
    "            history = kerasFFNN.fit(\n",
    "                x=X_train,\n",
    "                y=y_train,\n",
    "                epochs=num_epochs,\n",
    "                batch_size=64,\n",
    "                validation_split=0.1,\n",
    "                validation_data=(X_test, y_test),\n",
    "                verbose=0)\n",
    "            trn_elapsed = time.time() - start\n",
    "\n",
    "            train_accuracy = history.history['accuracy']\n",
    "            train_loss = history.history['loss']\n",
    "            v_accuracy = history.history['val_accuracy']\n",
    "            v_loss = history.history['val_loss']\n",
    "            \n",
    "            epoch_cnt   = range(len(train_accuracy)) # Get number of epochs\n",
    "            display_eval_graphs(ttl, epoch_cnt, train_accuracy, v_accuracy, train_loss, v_loss, \"Epochs\", \"Accuracy\", \"Loss\")\n",
    "\n",
    "            # Evaluate the model and determine final accuracy\n",
    "            start = time.time()\n",
    "            loss, test_accuracy = kerasFFNN.evaluate(x=X_test, y=y_test, verbose=0)\n",
    "            eval_elapsed = time.time() - start\n",
    "        \n",
    "            print(f\"Training time: {round(trn_elapsed, 2)} seconds\")\n",
    "            print(f\"Evaluation time: {round(eval_elapsed, 2)} seconds\")\n",
    "            print(f\"Accuracy: {round(test_accuracy * 100, 3)}, Loss: {round(loss, 5)}, Epochs: {num_epochs}, LR {learning_rate}\")\n",
    "            #run_result = { 'lr' : learning_rate, 'ep' : num_epochs, 'acc' : test_accuracy, 'loss' : loss, \\\n",
    "            #                       'ttime' : trn_elapsed }\n",
    "\n",
    "            y_test_proba = kerasFFNN.predict(X_test, batch_size=64, verbose=0)\n",
    "            # For classification: Convert probabilities to class labels\n",
    "            y_test_predict = np.argmax(y_test_proba, axis=1)\n",
    "            #print(\"Predictions (raw probabilities):\", y_test_proba.shape)\n",
    "            #print(\"Predicted Classes:\", y_test_predict.shape)\n",
    "            #for xx in range(15):\n",
    "            #    print(f\"{xx}: prob: {y_test_proba[xx]}\")\n",
    "            #for xx in range(15):\n",
    "            #    print(f\"{xx}: pred: {y_test_predict[xx]}\\ttrue: {y_test[xx]}\")                 \n",
    "            display_confusion_matrix(y_test, y_test_predict)\n",
    "\n",
    "            csv_row = np.array([fn, learning_rate, num_epochs, round(test_accuracy * 100, 2), \\\n",
    "                                round(loss, 5), round(trn_elapsed, 2)])\n",
    "            csv_data.append(csv_row)\n",
    "\n",
    "            %reset_selective -f history.history, train_accuracy, v_accuracy, train_loss, v_loss\n",
    "\n",
    "    npz_file.close()\n",
    "    \n",
    "    return csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5135e71-7806-4cf8-8080-a2dfcccc4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "filecol = 'fn'\n",
    "lr = 'lr'\n",
    "eps = 'epochs'\n",
    "ac = 'accuracy'\n",
    "ls = 'loss'\n",
    "tt = 'train time'\n",
    "\n",
    "def store_results(create_file, outfile, csv_data):\n",
    "    # Add the CSV data to a new or existing CSV file\n",
    "    df = pd.DataFrame(csv_info, columns=[filecol, lr, eps, ac, ls, tt])\n",
    "    if (create_file):\n",
    "        # write to file (create or overwrite)\n",
    "        #print(f\"Storing result in {outfile}\")\n",
    "        df.to_csv(outfile, index=False)\n",
    "        create_file = False\n",
    "    else:\n",
    "        # append to file\n",
    "        df.to_csv(outfile, mode='a', index=False, header=False)\n",
    "    return create_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1038e-2afe-42e2-99e2-91856a212030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_best(best_acc_indx, best_loss_indx):\n",
    "    print(f\"        best accuracy: Index {best_acc_indx}, least loss: Index {best_loss_indx}\")\n",
    "\n",
    "def display_row(index, row):\n",
    "    tbs = '\\t\\t'\n",
    "    if row[ls] >= 10.0:\n",
    "        tbs = '\\t'\n",
    "    print(f\"  {index}\\t{row[lr]:.3f}\\t\\t{int(row[eps]):03}\\t{row[ac]}\\t\\t{row[ls]:.5f}{tbs}{row[tt]} secs\")\n",
    "\n",
    "def display_results(csv_file):\n",
    "\n",
    "    #print(f\"Processing {outfile}\")\n",
    "\n",
    "    df = pd.read_csv(outfile)\n",
    "    \n",
    "    file_nm = \"\"\n",
    "    best_acc = 0.0\n",
    "    best_loss = 100.0\n",
    "    best_acc_indx = 0\n",
    "    best_loss_indx = 0\n",
    "    first_file = True\n",
    "\n",
    "    print(\"\\nFeed Forward Neural Network Comparison by Filename\")\n",
    "    for index, row in df.iterrows():\n",
    "        #print(f\"Index {index}: {row}\")\n",
    "        if not np.isnan(row['lr']):\n",
    "            if (row[filecol] != file_nm):\n",
    "                if (file_nm != \"\"):\n",
    "                    display_best(best_acc_indx, best_loss_indx)\n",
    "                    first_file = False\n",
    "                #print(f\"file_nm: {file_nm}, row[filecol]: {row[filecol]}\")\n",
    "                file_nm = row[filecol]\n",
    "                print(f\"\\n{file_nm]}\")\n",
    "                print(f\"  index\\t{lr}\\t\\t{eps}\\t{ac}\\t{ls}\\t\\t{tt}\")\n",
    "                display_row(index, row)\n",
    "                best_acc = 0.0\n",
    "                best_loss = 100.0\n",
    "            else:\n",
    "                display_row(index, row)\n",
    "                \n",
    "            if row[ac] > best_acc:\n",
    "                best_acc = row[ac]\n",
    "                best_acc_indx = index\n",
    "            if row[ls] < best_loss:\n",
    "                best_loss = row[ls]\n",
    "                best_loss_indx = index\n",
    "    # for last file\n",
    "    display_best(best_acc_indx, best_loss_indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4845ed50-a36d-4a63-a75f-6a607f65bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: 'chestmnist.npz', # image, label size length mismatch, not used here\n",
    "#files = ['bloodmnist.npz', 'breastmnist.npz', 'dermamnist.npz', 'octmnist.npz', 'organsmnist.npz', \\\n",
    "#         'nodulemnist3d.npz', 'organmnist3d.npz', 'retinamnist_128.npz']\n",
    "files = ['bloodmnist.npz', 'breastmnist.npz']\n",
    "#files = ['breastmnist.npz']\n",
    "\n",
    "start_run = time.time()\n",
    "\n",
    "rslt = {}\n",
    "first_write = True\n",
    "\n",
    "dtmin = get_time_with_minutes(get_localdate_str())\n",
    "outfile = \"FFNN-\" + dtmin +\".csv\"\n",
    "\n",
    "for fn in files:\n",
    "    file_to_load = \"G:/Continuing Education/Stanford/TECH 27/Project/\" + fn\n",
    "    csv_info = process_file(file_to_load, fn)\n",
    "\n",
    "    first_write = store_results(first_write, outfile, csv_info)\n",
    "\n",
    "end_run = time.time() - start_run\n",
    "\n",
    "display_results(outfile)\n",
    "\n",
    "if end_run > 180.0:\n",
    "    end_run_min = end_run / 60.0\n",
    "    end_run = end_run - end_run_min * 60.0\n",
    "    print(f\"\\nDONE: Total Run Time: {int(end_run_min)} minutes : {round(end_run, 2)} seconds\")\n",
    "else:\n",
    "    print(f\"\\nDONE: Total Run Time: {round(end_run, 2)} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
